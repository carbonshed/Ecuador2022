---
title: "gas transfer velocity wetland model"
author: "Kriddie"
date: "2023-12-10"
output: html_document
---

#mixed model for wetland data FLUX

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

stdCoef.merMod <- function(object) {
  sdy <- sd(getME(object,"y"))
  sdx <- apply(getME(object,"X"), 2, sd)
  sc <- fixef(object)*sdx/sdy
  se.fixef <- coef(summary(object))[,"Std. Error"]
  se <- se.fixef*sdx/sdy
  return(data.frame(stdcoef=sc, stdse=se))
}

library(here)
library(dplyr)
library(ggplot2)
library(lubridate)

library(lme4)
library(lmerTest)
library(jtools)
library(extraoperators)
library(JWileymisc)
library(multilevelTools)
library("Hmisc")
library(car)
library(GGally)
library(MCMCglmm)

```

# Referances and tutorials

> excellant tutorial, includes dragons
 https://ourcodingclub.github.io/tutorials/mixed-models/

> mulitvariate model building
 https://cran.r-project.org/web/packages/multilevelTools/vignettes/lmer-vignette.html

> article:https://peerj.com/articles/4794/
 https://peerj.com/articles/4794/

> use this to standardize regression coefficients in mixed effects model post-hoc
    # https://rdrr.io/github/gmonette/spida15/man/vif.lme.html
    
> to test for variance inflation
 https://rdrr.io/github/gmonette/spida15/man/vif.lme.html
 
 > Faq about mixed models 
 http://bbolker.github.io/mixedmodels-misc/glmmFAQ.html
#notes from stats consultant:

#need a thery befor applying transformations
test for homodesticity:
 https://www.r-bloggers.com/2016/01/how-to-detect-heteroscedasticity-and-rectify-it/
#normality of residuals: use shapiro wilk test
https://cran.r-project.org/web/packages/olsrr/vignettes/residual_diagnostics.html

#builing a model
  > put everything in one model 
  > step-wise is old-fashioned but people still do it
  > first think theoretically - and report all the effects, whether or not they are significant
  > don't add more than 20 predictors

#selecting a model:
 > Focus on your question, don’t just plug in and drop variables from a model haphazardly until you make something “significant”.
 > Always choose variables based on biology/ecology: I might use model selection to check a couple of non-focal parameters, but I keep the “core” of the model untouched in most cases. 
 > Define your goals and questions and focus on that. 
 > Also, don’t just put all possible variables in (i.e. don’t overfit). 
 > Remember that as a rule of thumb, you need 10 times more data than parameters you are trying to estimate.


#model interpretation!
 > Estimate is my co-efficient, if it is positive or negative is direction of relationship
  > In raw metric is can be easy to interpret – ie for each degree temperature  x ppm increase
  >	If you scale both, then beta cooefficient is comparable to one another	(rules of thumb something below point .3 is small)
   
##Mixed model with repeated measures
  > CO2 degassing is the response variable
  > Other variables are predictors (fixed effects)
  > Wetland site is random effect

##What's up with + (1|Wetland) ?:
with this notation we assume an intercept that’s different for each subject” … and “1” stands for the intercept here.You can think of this formula as telling your model that it should expect that there’s going to be multiple responses per subject, and these responses will depend on each subject’s baseline level. This effectively resolves the non-independence that stems from having multiple responses by the same subject.

##use log variables when you expect diminishing returns
 > transform regressors for normality and scale
 > adjust for scale can help with interpretation in some ways, but not necessarily. It depends on how you want to interpret the data
 > scaling won't change your results, but it may help you to interpret the data. If you have 2 predictor you can compare the 2 when you scale

#the assumption isn't on normality - the assumption is on the residculas
  > assumption is the *error* is  normaly distributed

#post-hoc tests: 
  > check for variance inflation vif()
  > among predictors, if there are high correlations between predictors, this can skew the effects
  > homoskedasticisty: check that error variance is normal and constant across the predictor variable
  > extract residuals - see that they are random

 

```{r load data}
#read in data
#df <- read.csv(here::here("Wetlands/Wetland_df_MERGE_2023-11-04.csv"))[c(17,2,6,14:16,18:45,52:59)]
df <- read.csv(here::here("Wetlands/Wetland_df_MERGE_2023-11-10.csv"))
df$num <- 1:44
```


###Predicted drivers of k

##drivers of gas transfer velocity
  #K increases with Air temperature (lower solubility) - (point, ave day, ave year)
  #K increases with Water temperature (lower solubility) - (ave day, ave year)
  #K is influenced by air-water diff (evaporative cooling or something)
  #K increases with surface area size (more fetch)
  #K increased with wind (more turbulance)
  #K increases with precipitation (more turbulance)
  #Solar radiation?
  
## All factors that were used in explaining CO2 and k

k increases with Air temperature - (ave day)
    > AirTemp_c

k increases with Water temperature - (ave day)
    > WaterTemp_c
    > WaterTemp_c_day
    
k has a relationship with the difference in water and air temp
  > Water_minus_air_Temp
  
k increases with surface area (more fetch)
    > Surface_area_m2

K increased with wind (more turbulance)
    > Precio_mm_ave2

K increased with wind (more turbulance)
  > windspeed_m_s

##CHecking Assumptions
#Step 1: Are there outliers in Y and X?
  > ‘outlier’ losley defined as an observation that sticks out from the rest. A more rigorous approach is to consider whether unusual observations exert undue influence on an analysis (e.g. on estimated parameters). We make a distinction between influential observations in the response variable and in the covariate
  > In regression-type techniques, outliers in the response variables are more complicated to deal with. Transforming the data is an option, but as the response variable is of primary interest, it is better to choose a statistical method that uses a probability distribution that allows greater variation for large mean values (e.g. gamma for continuous data; Poisson or negative binomial for count data) because doing this allows us to work with the original data. 
  > For multivariate analyses, this approach is not an option because these methods are not based on probability distributions. Instead, we can use a different measure of association. For example, the Euclidean distance is rather sensitive to large values because it is based on Pythagoras’ theorem, whereas the Chord distance down-weights large values (Legendre & Legendre 1998).
  > Ultimately, it is up to the ecologist to decide what to do with outliers.


```{r  assumptions step 1, echo=FALSE}
#Step 1: Are there outliers in Y and X?
 
#boxplots galore
boxplot(df$k_m.d) ##NOT transformed
hist(df$k_m.d)
ggplot(df, aes(k_m.d, num)) + geom_point()

boxplot(df$AirTemp_c)  # one?? - checked, it is as legit as the data collected can be. 
hist(df$AirTemp_c)
ggplot(df, aes(AirTemp_c, num)) + geom_point() #this looks pretty good

boxplot(df$Baro_kpa_day) #maybe use this data because no big outliers?
hist(df$BaroTemp_c_day)
ggplot(df, aes(BaroTemp_c_day, num)) + geom_point()

boxplot(df$Watertemp_c)
hist(df$Watertemp_c)
ggplot(df, aes(Watertemp_c, num)) + geom_point()

boxplot(df$waterTemp_c_day) #one? - checked, it is legit
hist(df$waterTemp_c_day)
ggplot(df, aes(waterTemp_c_day, num)) + geom_point()

boxplot(df$Water_minus_air_Temp) #one outlier
hist(df$Water_minus_air_Temp) 
ggplot(df, aes(Water_minus_air_Temp, num)) + geom_point()# this looks good

boxplot(df$Baro_Water_minus_air_Temp) #some outliers
hist(df$Baro_Water_minus_air_Temp)#this looks good
ggplot(df, aes(Baro_Water_minus_air_Temp, num)) + geom_point()

boxplot(log(df$surface_area_m2)) #seems like log transform is the way to go here. looks good after log transform
hist(log(df$surface_area_m2))
ggplot(df, aes(log(surface_area_m2), num)) + geom_point()

boxplot(df$PrecipAccuDay_mm) 
hist(df$PrecipAccuDay_mm)
ggplot(df, aes(PrecipAccuDay_mm, num)) + geom_point()

boxplot(df$solarrad_W_m2) 
hist(df$solarrad_W_m2)
ggplot(df, aes(solarrad_W_m2, num)) + geom_point()

boxplot(df$windspeed_m_s) 
hist(df$windspeed_m_s)
ggplot(df, aes(windspeed_m_s, num)) + geom_point()
```

#Finding from step 1.
  > there are outliers but I checked, and these measurments are legit (I did change a few AirTemp_c measurments)
  > I think I will just leave the rest as-is. I denfinatly don't want to delete data since it is so precious. But perhapes I should just keep this in mind for analysis...
  >waterTemp_c_day - one outlier
  > waterTemp_c_yearly- wetland 5 is outlier
  > Water_minus_air_Temp - one outlier (wetland05), but dotplot looks ok
  > surface_area_m2 - transform

  
##checking Assumptions
#Step 5: Is there collinearity among the covariates? (we skip step 4)
   > Collinearity is the existence of correlation between covariates.
  > If the underlying question in a study is which covariates are driving the response variable(s), then the biggest problem to overcome is often collinearity. 
  > Collinearity among predictors can cause several problems in model interpretation because those predictors explain some of the same variance in the response variable, and their effects cannot be estimated independently 
  > If collinearity is ignored, one is likely to end up with a confusing statistical analysis in which nothing is significant, but where dropping one covariate can make the others significant, or even change the sign of estimated parameters. 
  > it can cause model convergence issues as models struggle to partition variance between predictor variables. 
  > positively correlated variables can have negatively correlated regression coefficients, as the marginal effect of one is estimated, given the effect of the other, leading to incorrect interpretations of the direction of effects
  > collinearity can inflate standard errors of coefficient estimates and make ‘true’ effects harder to detect (Zuur, Ieno & Elphick, 2010).
  > can affect the accuracy of model averaged parameter estimates during multi-model inference
  > The price one pays for this situation is that the standard errors of the parameters are inflated with the square root of 1/(1 − Rj2), also called the variance inflation factor (VIF), which means that the P-values get larger making it more difficult to detect an effect.

#Detecting Collinearity: 
  > creating correlation matrices between raw explanatory variables, with *values > 0.7* suggesting both should not be used in the same model (Dormann et al., 2013)
  > calculating the variance inflation factor (VIF) of each predictor that is a candidate for inclusion in a model (details in Zuur, Ieno & Elphick, 2010) and dropping variables with a VIF higher than a certain value (e.g. 3, Zuur, Ieno & Elphick, 2010; or 10, Quinn & Keough, 2002). Sequentially drop the covariate with the highest VIF, recalculate the VIFs and repeat this process until all VIFs are smaller than a pre-selected threshold. *Montgomery & Peck (1992) used a value of 10, but a more stringent approach is to use values as low as 3 as we did here.*
    > High, or even moderate, collinearity is especially problematic when ecological signals are weak. In that case, even a VIF of 2 may cause nonsignificant parameter estimates, compared to the situation without collinearity. One problem with these methods though is that they rely on a user-selected choice of threshold of either the correlation coefficient or the VIF, and use of more stringent (lower) is probably sensible. 
  > Other ways to detect collinearity include pairwise scatterplots comparing covariates, correlation coefficients or a PCA biplot (J.olliffe 2002) applied on all covariates. 
  > Some argue that one should always prefer inspection of VIF values over correlation coefficients of raw predictors because strong multicollinearity can be hard to detect with the latter. 
  

#solutions  
  > The easiest way to solve collinearity is by dropping collinear covariates. The choice of which covariates to drop can be based on the VIFs, or perhaps better, on common sense or biological knowledge. 
  > select one variable as representative of multiple collinear variables (Austin, 2002), ideally using biological knowledge/reasoning to select the most meaningful variable (Zuur, Ieno & Elphick, 2010); 
  > or conduct a dimension-reduction analysis (e.g. Principal Components Analysis; James & McCullugh, 1990), leaving a single variable that accounts for most of the shared variance among the correlated variables. Both approaches will only be applicable if it is possible to group explanatory variables by common features, thereby effectively creating broader, but still meaningful explanatory categories. For instance, by using mass and body length metrics to create a ‘scaled mass index’ representative of body size (Peig & Green, 2009).
  > An alternative consideration is how easy alternative covariates are to measure in terms of effort and cost. Whenever two covariates X and Z are collinear, and Z is used in the statistical analysis, then the biological discussion in which the effect of Z is explained should include mention of the collinearity, and recognize that it might well be X that is driving the system (cf. Gjerdrum et al. 2008). 


```{r  assumptions step 5.1, echo=FALSE}
#make correlation matrix

df_2 <- df[,c("Watertemp_c",
            #"AirTemp_c",
             # "BaroTemp_c_day",
            "Baro_Water_minus_air_Temp",
              "waterTemp_c_day",
              "Water_minus_air_Temp",
              "surface_area_m2",
              "Watershed_m2",
              "PrecipAccuDay_mm",
              "solarrad_Wm2_daymean",
              "windspeed_m_s"
              )]

cor(df_2, use = "complete.obs")
res2 <- rcorr(as.matrix(df_2))
res2$r
res2$P
correlation_pvalue <- res2$P
correlation_coefficiant <- res2$r



```

##findings:
  > remove AirTemp_c (I don't really trust this data.. just that the sensor might have been heated up by the sun.. or accidently stored next to our bodies. We have this data because we wanted barometric pressure, so we wern't careful about potential airtemp data) Also it coovariates strongly with other variable
  > remove Watertemp_day because it is correlates strongly with WaterTemp_c and I like using water temp at that instant of measurment more
  > remove barotemp dat because ir coorelates with water_minus_air_temp too much
 
    *now that we've removed factors, all correlation matrices between raw explanatory variables have values < 0.7*

```{r  assumptions step 5.2, echo=FALSE}

#calculating the variance inflation factor (VIF) of each predictor that is a candidate for inclusion in a model
    #The value for VIF starts at 1 and has no upper limit. A general rule of thumb for interpreting VIFs is as follows:
        #A value of 1 indicates there is no correlation between a given predictor variable and any other predictor variables in the model.
        #A value between 1 and 5 indicates moderate correlation between a given predictor variable and other predictor variables in the model, but this is often not severe enough to require attention.
        #A value greater than 5 indicates potentially severe correlation between a given predictor variable and other predictor variables in the model. In this case, the coefficient estimates and p-values in the regression output are likely unreliable.


M1 <- lmer(k_m.d ~ scale(Watertemp_c) +
                  scale(Water_minus_air_Temp) +
                   scale(Baro_Water_minus_air_Temp) +
                  scale(log(surface_area_m2)) +
                  scale(PrecipAccuDay_mm) + 
                  scale(solarrad_Wm2_daymean) +
                  scale(windspeed_m_s) +
                  (1 |Wetland), data =df)

vif(M1)

  
```

#findings from step 5.2
  > BaroTemp_c_yearly had the highest VIF values, but once I got rid of Watershed_m2 the VIF droped for all variables. I need to get rid of this varliable because I don't have it for all wetlands.
  > this is also the case for SA_to_Vol_ratio, but I am only missing wetland 12 in this case, so I'm tempted to keep it and do 2 analysis



#checking Assumptions
##Step 2: Do we have homogeneity of variance?
    
    *we have to check this after we run the model*

  > Homogeneity of variance is an important assumption in analysis of variance (ANOVA), other regression-related models and in multivariate techniques like discriminant analysis
  > For a simplistic linear regression model heterogeneity seriously degrades the least-square estimators when the ratio between the largest and smallest variance is 4 (conservative) or more.
  
##detection
  > In regression-type models, verification of homogeneity should be done using the *residuals of the model*; i.e. by plotting residuals vs. fitted values, and making a similar set of conditional boxplots for the residuals. In all these graphs the residual variation should be similar. 
  
##solution
  > The solution to heterogeneity of variance is either a transformation of the response variable to stabilize the variance, or applying statistical techniques that do not require homogeneity (e.g. generalized least squares; Pinheiro & Bates 2000; Zuur et al. 2009a).


```{r  assumptions step 2, echo=FALSE}
##Model 1
#plot residuals 
res1 <- resid(M1)
#produce residual vs. fitted plot
plot(fitted(M1), res1)
#add a horizontal line at 0 
abline(0,0)
#create Q-Q plot for residuals
qqnorm(res1)
#add a straight diagonal line to the plot
qqline(res1) 


```

#Findings from  assumptions step 2
> the residual plots look good!
> the q plots don't look as good
> model 2 (no SA_to_Vol_ratio) is better than model 1

##checking Assumptions
#Step 3: Are the data normally distributed?

  *we have to check this after we run the model*

  > Linear regression assumes normality, but is reasonably robust against violation of the assumption (Fitzmaurice, Laird & Ware 2004). 
  > In linear regression, we actually assume normality of all the replicate observations at a particular covariate value (Fig. 6; Montgomery & Peck 1992), an assumption that cannot be verified unless one has many replicates at each sampled covariate value. 
  > However, normality of the raw data implies normality of the residuals. Therefore, we can make histograms of residuals to get some impression of normality (Quinn & Keough 2002; Zuur et al. 2007), even though we cannot fully test the assumption


```{r  assumptions step 3, echo=FALSE}
#Create density plot of residuals
plot(density(res1))

#histagram
hist(res1)

# create pairs plot 
#ggpairs( df_2 ) #this is too much, but maybe with a different dataset it would be useful

```

#Findings from  assumptions step 3
> looks good ro me!
> model 2 (no SA_to_Vol_ratio) is better than model 1


```{r  model building, echo=FALSE}

#I need to remove REML (read about this more so that you understand)

M1 <- lmer(k_m.d ~ scale(Watertemp_c) +
                 scale(Water_minus_air_Temp) +
                 #  scale(Baro_Water_minus_air_Temp) +
                  scale(log(surface_area_m2)) +
                  scale(PrecipAccuDay_mm) + 
                  scale(solarrad_W_m2) +
                  scale(windspeed_m_s) +
                  (1 |Wetland), data =df,REML=FALSE)

summary(M1)
modelPerformance(M1)
```



```{r  k model building3, echo=FALSE}


#remove Wetland 12 for this analysis?
df_1 <- df%>%filter(Wetland!="Wetland12")

Dataframe <- data.frame("Model"= as.character(), "AIC" = as.numeric(),"BIC" = as.numeric())

n = 7
grid <- expand.grid(replicate(n, 0:1, simplify = FALSE))
grid_df <- as.data.frame(grid$Var1)
colnames(grid_df) <- "Var1"
grid_df$Var2 <- grid$Var2
grid_df$Var3 <- grid$Var3
grid_df$Var4 <- grid$Var4
grid_df$Var5 <- grid$Var5
grid_df$Var6 <- grid$Var6
grid_df$Var7 <- grid$Var7

grid_df$Var1[which(grid_df$Var1 == 1)] <- "Watertemp_c"
grid_df$Var2[which(grid_df$Var2 == 1)] <- "Baro_Water_minus_air_Temp"
grid_df$Var3[which(grid_df$Var3 == 1)] <- "log(surface_area_m2)"
grid_df$Var4[which(grid_df$Var4 == 1)] <- "PrecipAccuDay_mm"
grid_df$Var5[which(grid_df$Var5 == 1)] <- "solarrad_Wm2_daymean"
grid_df$Var6[which(grid_df$Var6 == 1)] <- "windspeed_m_s"
grid_df$Var7[which(grid_df$Var7 == 1)] <- "SA_to_Vol_ratio"

grid_df[grid_df == 0] <- NA

for (row in 2:nrow(grid_df)){
  f <- grid_df[row, ]
  f_2 <- f[!is.na(f)]
  
  if(length(f_2)==1){
    model <- lmer(
    as.formula(paste0("k_m.d ~ ","scale(",f_2[1],") + (1 |Wetland)")),
    data = df_1,REML = FALSE)
    summary <- summary(model)
    
    Dataframe[nrow(Dataframe) + 1,] <- list(toString(rownames(summary$coefficients)), summary$AIC[1], summary$AIC[2])
  
      } else if(length(f_2)==2) { 
    model <- lmer(
      as.formula(paste0("k_m.d ~ ","scale(",f_2[1],") +scale(",f_2[2],") + (1 |Wetland)")),data=df_1,REML=FALSE)
    summary <- summary(model)
    
    Dataframe[nrow(Dataframe) + 1,] <- list(toString(rownames(summary$coefficients)), summary$AIC[1], summary$AIC[2])
       
      } else if(length(f_2)==3) { 
    model <- lmer(
      as.formula(paste0("k_m.d ~ ","scale(",f_2[1],") +scale(",f_2[2],") + scale(",f_2[3],") + (1 |Wetland)")),data=df_1,REML=FALSE)
    summary <- summary(model)
    
    Dataframe[nrow(Dataframe) + 1,] <- list(toString(rownames(summary$coefficients)), summary$AIC[1], summary$AIC[2])
       
      } else if(length(f_2)==4) { 
    model <- lmer(
      as.formula(paste0("k_m.d ~ ","scale(",f_2[1],") +scale(",f_2[2],") + scale(",f_2[3],") + scale(",f_2[4],") + (1 |Wetland)")),data=df_1,REML=FALSE)
    summary <- summary(model)
    
    Dataframe[nrow(Dataframe) + 1,] <- list(toString(rownames(summary$coefficients)), summary$AIC[1], summary$AIC[2])
       
      } else if(length(f_2)==5) { 
    model <- lmer(
      as.formula(paste0("k_m.d~ ","scale(",f_2[1],") +scale(",f_2[2],") + scale(",f_2[3],") + scale(",f_2[4],") + scale(",f_2[5],") + (1 |Wetland)")),data=df_1,REML=FALSE)
    summary <- summary(model)
    
    Dataframe[nrow(Dataframe) + 1,] <- list(toString(rownames(summary$coefficients)), summary$AIC[1], summary$AIC[2])
       
      } else if(length(f_2)==6) { 
    model <- lmer(
      as.formula(paste0("k_m.d ~ ","scale(",f_2[1],") +scale(",f_2[2],") + scale(",f_2[3],") + scale(",f_2[4],") + scale(",f_2[5],") + scale(",f_2[6],") + (1 |Wetland)")),data=df_1,REML=FALSE)
    summary <- summary(model)
    
    Dataframe[nrow(Dataframe) + 1,] <- list(toString(rownames(summary$coefficients)), summary$AIC[1], summary$AIC[2])
      } else if(length(f_2)==7) { 
    model <- lmer(
      as.formula(paste0("k_m.d ~ ","scale(",f_2[1],") +scale(",f_2[2],") + scale(",f_2[3],") + scale(",f_2[4],") + scale(",f_2[5],") + scale(",f_2[6],") + scale(",f_2[7],") + (1 |Wetland)")),data=df_1,REML=FALSE)
    summary <- summary(model)
    
    Dataframe[nrow(Dataframe) + 1,] <- list(toString(rownames(summary$coefficients)), summary$AIC[1], summary$AIC[2])
       }
}
```


#k600

```{r  k600 model building3, echo=FALSE}


#remove Wetland 12 for this analysis?
df_1 <- df#%>%filter(Wetland!="Wetland12")

Dataframe <- data.frame("Model"= as.character(), "AIC" = as.numeric(),"BIC" = as.numeric())

n = 6
grid <- expand.grid(replicate(n, 0:1, simplify = FALSE))
grid_df <- as.data.frame(grid$Var1)
colnames(grid_df) <- "Var1"
grid_df$Var2 <- grid$Var2
grid_df$Var3 <- grid$Var3
grid_df$Var4 <- grid$Var4
grid_df$Var5 <- grid$Var5
grid_df$Var6 <- grid$Var6
#grid_df$Var7 <- grid$Var7

grid_df$Var1[which(grid_df$Var1 == 1)] <- "Watertemp_c"
grid_df$Var2[which(grid_df$Var2 == 1)] <- "Baro_Water_minus_air_Temp"
grid_df$Var3[which(grid_df$Var3 == 1)] <- "log(surface_area_m2)"
grid_df$Var4[which(grid_df$Var4 == 1)] <- "PrecipAccuDay_mm"
grid_df$Var5[which(grid_df$Var5 == 1)] <- "solarrad_W_m2"
grid_df$Var6[which(grid_df$Var6 == 1)] <- "windspeed_m_s"
#grid_df$Var7[which(grid_df$Var7 == 1)] <- "SA_to_Vol_ratio"

grid_df[grid_df == 0] <- NA

for (row in 2:nrow(grid_df)){
  f <- grid_df[row, ]
  f_2 <- f[!is.na(f)]
  
  if(length(f_2)==1){
    model <- lmer(
    as.formula(paste0("K600 ~ ","scale(",f_2[1],") + (1 |Wetland)")),
    data = df_1,REML = FALSE)
    summary <- summary(model)
    
    Dataframe[nrow(Dataframe) + 1,] <- list(toString(rownames(summary$coefficients)), summary$AIC[1], summary$AIC[2])
  
      } else if(length(f_2)==2) { 
    model <- lmer(
      as.formula(paste0("K600 ~ ","scale(",f_2[1],") +scale(",f_2[2],") + (1 |Wetland)")),data=df_1,REML=FALSE)
    summary <- summary(model)
    
    Dataframe[nrow(Dataframe) + 1,] <- list(toString(rownames(summary$coefficients)), summary$AIC[1], summary$AIC[2])
       
      } else if(length(f_2)==3) { 
    model <- lmer(
      as.formula(paste0("K600 ~ ","scale(",f_2[1],") +scale(",f_2[2],") + scale(",f_2[3],") + (1 |Wetland)")),data=df_1,REML=FALSE)
    summary <- summary(model)
    
    Dataframe[nrow(Dataframe) + 1,] <- list(toString(rownames(summary$coefficients)), summary$AIC[1], summary$AIC[2])
       
      } else if(length(f_2)==4) { 
    model <- lmer(
      as.formula(paste0("K600 ~ ","scale(",f_2[1],") +scale(",f_2[2],") + scale(",f_2[3],") + scale(",f_2[4],") + (1 |Wetland)")),data=df_1,REML=FALSE)
    summary <- summary(model)
    
    Dataframe[nrow(Dataframe) + 1,] <- list(toString(rownames(summary$coefficients)), summary$AIC[1], summary$AIC[2])
       
      } else if(length(f_2)==5) { 
    model <- lmer(
      as.formula(paste0("K600 ~ ","scale(",f_2[1],") +scale(",f_2[2],") + scale(",f_2[3],") + scale(",f_2[4],") + scale(",f_2[5],") + (1 |Wetland)")),data=df_1,REML=FALSE)
    summary <- summary(model)
    
    Dataframe[nrow(Dataframe) + 1,] <- list(toString(rownames(summary$coefficients)), summary$AIC[1], summary$AIC[2])
       
      } else if(length(f_2)==6) { 
    model <- lmer(
      as.formula(paste0("K600 ~ ","scale(",f_2[1],") +scale(",f_2[2],") + scale(",f_2[3],") + scale(",f_2[4],") + scale(",f_2[5],") + scale(",f_2[6],") + (1 |Wetland)")),data=df_1,REML=FALSE)
    summary <- summary(model)
    
    Dataframe[nrow(Dataframe) + 1,] <- list(toString(rownames(summary$coefficients)), summary$AIC[1], summary$AIC[2])
      } else { 
    model <- lmer(
      as.formula(paste0("K600 ~ ","scale(",f_2[1],") +scale(",f_2[2],") + scale(",f_2[3],") + scale(",f_2[4],") + scale(",f_2[5],") + scale(",f_2[6],") + scale(",f_2[7],") + (1 |Wetland)")),data=df_1,REML=FALSE)
    summary <- summary(model)
    
    Dataframe[nrow(Dataframe) + 1,] <- list(toString(rownames(summary$coefficients)), summary$AIC[1], summary$AIC[2])
       }
}
```

#run best models according to AIC and BIC
```{r  model building, echo=FALSE}


#with and without wetland 12
#Water_minus_air_Temp


df_1 <- df#%>%filter(Wetland!="Wetland12")

#AIC & BIC are same 
df_1$K600
M_AIC <- lmer(K600 ~ 
                 # scale(Water_minus_air_Temp)+
                    scale(solarrad_W_m2) +
                   scale(Baro_Water_minus_air_Temp) +
                  (1 |Wetland), data =df_1,REML = FALSE)
summary(M_AIC)
modelPerformance(M_AIC)


```

```{r  visualize, echo = FALSE}

library(ggeffects)  # install the package first if you haven't already, then load it

# Extract the prediction data frame
pred.mm <- ggpredict(M_AIC, terms = c("Water_minus_air_Temp" ))  # this gives overall predictions for the model
# Plot the predictions 

(ggplot(pred.mm) + 
   geom_line(aes(x = x, y = predicted)) +          # slope
   geom_ribbon(aes(x = x, ymin = predicted - std.error, ymax = predicted + std.error), 
               fill = "lightgrey", alpha = 0.5) +  # error band
   geom_point(data = df_1,                      # adding the raw data (scaled values)
              aes(x = Water_minus_air_Temp, y = K600, colour = Wetland), size = 3) + 
   labs(x = "Water - air temperature", y = "k600", 
        title = "k600 model") + 
   theme_minimal()
)


sjPlot::tab_model(M_AIC, 
                  show.re.var= TRUE, 
                  pred.labels =c("(Intercept)", "Air - water temperature (c)"),
                  dv.labels= "k600 prediction model")
```

